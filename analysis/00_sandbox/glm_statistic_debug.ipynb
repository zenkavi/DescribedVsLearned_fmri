{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6616a866-9140-4178-ac95-2527518f1b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import glob\n",
    "import os\n",
    "from pyhere import here\n",
    "import sys\n",
    "\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.plotting import plot_stat_map, view_img, plot_glass_brain\n",
    "from nilearn.image import concat_imgs, smooth_img, mean_img, math_img, resample_to_img\n",
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "from nilearn.glm import threshold_stats_img\n",
    "from nilearn.glm.second_level import non_parametric_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b719bd29-4eae-4ce7-9968-14783923f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(str(here('01_level1')))\n",
    "from level1_utils import make_level1_design_matrix, make_contrasts\n",
    "from utils import get_from_sidecar, get_model_regs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403f2502-31ed-4823-bee5-36fb5bf0fb67",
   "metadata": {},
   "source": [
    "## How can l2 z-maps look very constrained but l3 randomise with massive swaths of activation?\n",
    "\n",
    "- How l3 t values be larger than the largest t values in the individual l2 mean maps?\n",
    "    - L2 mean maps are generated by `nilearn.glm.second_level.SecondLevelModel.compute_contrast` with argument `output_type=\"z_score\"`\n",
    "    - L3 t values are computed using FSL's `randomise`  \n",
    "\n",
    "Previous hypotheses:\n",
    "\n",
    "- Is it because `rabndomise` uses enhanced TFCE statistics instaed of raw T stats?  \n",
    "    - No. When I tried voxelwise setting `tfce=False` and checked the min and max values in `randomise_results.outputs.tstat_files` they were the same as the outputs using tfce.  \n",
    "\n",
    "- Is it because of smoothing?\n",
    "    - No. Smoothing l2 images made values only smaller.\n",
    "\n",
    "Current hypothesis:\n",
    "\n",
    "- Statistic mismatch between levels of analysis.\n",
    "    - FSL's `randomise` expects `cope` images. These should be beta weights. What `output_type` do these correspond to in `nilearn` terminology?\n",
    "    - What is the correct input type for l2 and l3 analyses in `nilearn`?\n",
    "    - What does a max T distribution look like in `nilearn`s permutation testing framework?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b038cf9-0082-4c39-8313-a9e9895122a7",
   "metadata": {},
   "source": [
    "## `compute_contrast` output types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391ecdf-d747-4f5c-88ec-f7aa9d0e3282",
   "metadata": {},
   "source": [
    "Based on the [`Contrast` class](https://github.com/nilearn/nilearn/blob/2fd6665664f27dfbf1f10aeeba660adc9b560361/nilearn/glm/contrasts.py#L143)\n",
    " in `nilearn.glm.contrasts`\n",
    "\n",
    "- `effect_size`: parameter estimate from the GLM defined as [`effect = np.dot(matrix, self.theta)`](https://github.com/nilearn/nilearn/blob/2fd6665664f27dfbf1f10aeeba660adc9b560361/nilearn/glm/model.py#L213)\n",
    "- `effect_variance`: variance of the parameter estimate from the GLM [`sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))`](https://github.com/nilearn/nilearn/blob/2fd6665664f27dfbf1f10aeeba660adc9b560361/nilearn/glm/model.py#L213)\n",
    "- `stat`: decision statistic[`stat = (self.effect - baseline) / np.sqrt(self.variance)`](https://github.com/nilearn/nilearn/blob/2fd6665664f27dfbf1f10aeeba660adc9b560361/nilearn/glm/contrasts.py#L223) comparing the parameter estimate to the null hypothesis (i.e. is it different than 0.)\n",
    "- `p_value`: p value for the decision statistic [`p_values = scipy.stats.t.sf(self.stat_, self.dof`](https://github.com/nilearn/nilearn/blob/2fd6665664f27dfbf1f10aeeba660adc9b560361/nilearn/glm/contrasts.py#L275)\n",
    "- `z_score`: z scores for the given p values. Almost identical to `stat`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68cf855-a3b8-49c1-9a05-465aa8b7b6e9",
   "metadata": {},
   "source": [
    "- Run level 1 analysis on single subject single run\n",
    "- Run `compute_contrast` for each `output_type`\n",
    "- Pick a single voxel\n",
    "- Run GLM on it and compare the value for this voxel in each contrast map to the GLM summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74e4095-204a-47e7-8114-92c0b903fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subnum = '01'\n",
    "runnum = '1'\n",
    "mnum = 'model3'\n",
    "\n",
    "data_path = '/Users/zeynepenkavi/Downloads/GTavares_2017_arbitration/bids_nifti_wface/'\n",
    "behavior_path = '/Users/zeynepenkavi/Downloads/GTavares_2017_arbitration/behavioral_data/all_trials.csv'\n",
    "\n",
    "noise_model='ar1'\n",
    "hrf_model='spm'\n",
    "drift_model='cosine'\n",
    "smoothing_fwhm=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e474af7d-ca11-4049-a3ea-83d48927c550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 'modulation' column was found in the given events data and is used.\n"
     ]
    }
   ],
   "source": [
    "design_matrix= make_level1_design_matrix(subnum, runnum, mnum, data_path, behavior_path, regress_rt = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c3bc0f-ac57-43c2-a6ea-ca87df6c34cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_events = os.path.join(data_path, 'sub-%s/func/sub-%s_task-bundles_run-%s_events.tsv'%(subnum, subnum, runnum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9184afd3-fd18-4268-9942-763000a51731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fmri_img: path to preproc_bold that the model will be fit on\n",
    "fmri_img = os.path.join(data_path,\"derivatives/fmriprep/sub-%s/func/sub-%s_task-bundles_run-%s_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz\"%(subnum, subnum, runnum))\n",
    "os.path.isfile(fmri_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2cfd0ff-d3de-45d0-863f-221afd957266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in preproc_bold for that run\n",
    "cur_img_tr = get_from_sidecar(subnum, runnum, 'RepetitionTime', data_path)\n",
    "\n",
    "#read in events.tsv for that run\n",
    "cur_events = pd.read_csv(run_events, sep = '\\t')\n",
    "\n",
    "mask_img = nib.load(os.path.join(data_path,'derivatives/fmriprep/sub-%s/func/sub-%s_task-bundles_run-%s_space-MNI152NLin2009cAsym_res-2_desc-brain_mask.nii.gz'%(subnum, subnum, runnum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a144992-de9c-4897-89b6-8cb0df316d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_glm = FirstLevelModel(t_r=cur_img_tr,\n",
    "                       noise_model=noise_model,\n",
    "                       standardize=False,\n",
    "                       hrf_model=hrf_model,\n",
    "                       drift_model=drift_model,\n",
    "                       smoothing_fwhm=smoothing_fwhm,\n",
    "                       mask_img=mask_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1881c3b-ddb6-4b31-b748-d16cf2ae1f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_glm = fmri_glm.fit(fmri_img, design_matrices = design_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b75e0ddf-7509-473b-9c3e-a11d67b51a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrasts = make_contrasts(design_matrix, mnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1be3015-85b5-4c65-a905-c3b53e37f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_val = contrasts['reward_par']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66f793b8-0fc8-4a75-85a6-a87c59c2e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_map = fmri_glm.compute_contrast(contrast_val, output_type='z_score')\n",
    "stat_map = fmri_glm.compute_contrast(contrast_val, output_type='stat')\n",
    "p_map = fmri_glm.compute_contrast(contrast_val, output_type='p_value')\n",
    "effect_size_map = fmri_glm.compute_contrast(contrast_val, output_type='effect_size')\n",
    "# effect_variance_map = fmri_glm.compute_contrast(contrast_val, output_type='effect_variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58126e8d-cd43-4b8f-8924-c84a4cbd8ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_map_data = p_map.get_fdata()\n",
    "oneminp_data = 1 - p_map_data\n",
    "oneminp_map = nib.Nifti1Image(oneminp_data.astype(np.float64), p_map.affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "91f915d0-c41d-47fa-bfbc-1df6f2519389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 83, 32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick a voxel, the one with the max statistic\n",
    "np.unravel_index(np.argmax(stat_map.get_fdata()), stat_map.get_fdata().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa50d47d-14fb-4f00-8225-1ca93cfcb4cf",
   "metadata": {},
   "source": [
    "## Input type for level2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e8eca1-074f-4b5a-83e9-941835b3f08b",
   "metadata": {},
   "source": [
    "## Run level3 analysis using nilearn instead of randomise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6542da-b3cc-4421-b98f-1f8de83be2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1's for all runs to get average for subject per contrast\n",
    "design_matrix = pd.DataFrame([1] * len(l2_imgs), columns=['intercept'])\n",
    "model = SecondLevelModel(smoothing_fwhm=5.0)\n",
    "\n",
    "model = model.fit(l2_imgs, design_matrix=design_matrix)\n",
    "\n",
    "# z_map = model.compute_contrast(output_type='z_score')\n",
    "z_map = model.compute_contrast(output_type='stat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c7f4dd-eb97-449c-a973-9051af3d90aa",
   "metadata": {},
   "source": [
    "Raw map thresholded at an arbitrary large value. What nilearn reports for `output_type = \"stat\"` is comparable to randomise outputs.\n",
    "\n",
    "What statistic is reported when `output_type = \"stat\"`? \n",
    "\n",
    "Could this be why things look weird when using randomise? I'm feeding in `z_score` maps insteaf og `stat` maps? What input does randomise expect? `cope` images, contrast of parameters (beta weights)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865fe1a-be90-48b4-b0f3-c7f6ea57a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stat_map(z_map, threshold=3, draw_cross=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076620be-2584-43b1-b452-b9187fdb0d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_log_pvals_permuted_ols_unmasked = non_parametric_inference(l2_imgs,\n",
    "                             design_matrix=design_matrix,\n",
    "                             model_intercept=True, n_perm=5000,\n",
    "                             two_sided_test=False,\n",
    "                             smoothing_fwhm=5.0, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed76d36-607d-4d58-b02c-f5f41e6ee17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stat_map(\n",
    "    neg_log_pvals_permuted_ols_unmasked, colorbar=True, \n",
    "    threshold=1, draw_cross=False, cut_coords = (-6, -95, 14))\n",
    "# The neg-log p-values obtained with non parametric testing are capped at 3\n",
    "# when the number of permutations is 1e3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfe509c-7f37-4c0e-aa3e-5e20b98d5a8a",
   "metadata": {},
   "source": [
    "What is the max T distribution that lead to the map above? Should be able to get it with the `nilearn.mass_univariate.permuted_ols` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6797376f-17de-4711-b223-c04b379ac166",
   "metadata": {},
   "source": [
    "### Compared to the randomise output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd9b5a1-7ac2-48f3-9843-e1289feb1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stat_map(os.path.join(l3_path, 'rand_tfce_tstat1_pos_overall-mean_reward_par_model3_reg-rt0.nii.gz'), threshold = 4.6, draw_cross=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
